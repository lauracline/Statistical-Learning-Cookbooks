---
title: "Ridge and Lasso Regression"
author: "Laura Cline"
date: "24/10/2021"
output:
  pdf_document:
    toc: yes
  word_document:
    toc: yes
  html_document:
    toc: yes
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(glmnet) #glmnet() function for ridge/lasso
library(ISLR) # Hitters data
library(pls) #pcr() function for principal components regression
```

We will use the `glmnet` package in order to perform ridge regression and the lasso. The main function in this package is `glmnet()`, which can be used to fit ridge regression models, lasso models, and more. This function has slightly different syntax from other model-fitting functions that we have encountered thus far. In particular, we must pass in an `x` matrix as well as a `y` vector, and we do not use the `y ~ x` syntax. We will now perform ridge regression and the lasso in order to predict `Salary` on the `Hitters` data. Before proceeding, ensure that the missing values have been removed from the data. 

```{r}
# Prepare the Hitters data as done in the previous lab
hitters <- Hitters
hitters <- na.omit(hitters)
attach(hitters)
colnames(hitters) <- tolower(colnames(hitters))
```

The `model.matrix()` function is particularly useful for creating `x`; not only does it produce a matrix corresponding to the 19 predictors but it also automatically transforms any qualitative variables into dummy variables. The latter property is important because `glmnet()` can only take numerical, quanitative inputs. 

# Ridge Regression

The `glmnet()` function has an alpha argument that determines what type of model is fit. If `alpha=0` then a ridge regression model is fit, and if `alpha=1` then a lasso model is fit. We first fit a ridge regression model. 

Before continuing, take a look at the `glmnet()` vignette to get an idea of what we need beforehand. 

* `x` - input matrix containing rows of observations 

* `y` - response variable matrix

* `lamba` - a user created decreasing sequence of lambda values

```{r}
# Lambda will be a vector of length 100, ranging from 10^10 to 10^-2
x <- model.matrix(salary~., hitters)[,-1]
y <- hitters$salary
grid <-10^seq(10,-2,length=100) 
ridge.mod <- glmnet(x,y, alpha=0, lambda=grid)
```

Because we supplied a 100 lambda, we have got a coefficient matrix that is 20x100. We expect that the coefficients of models with larger lambdas will be much smaller than those with smaller lambdas. Remember, the sequence is decreasing so the further along the sequence the larger the coefficients. 

By default, the `glmnet()` function performs ridge regression for an automatically selected range of $\lambda$ values. However, here we have chosen to implement the function over a grid of values ranging from $\lambda = 10^{10}$ to $\lambda = 10^{-2}$, especially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit. As we will see, we can also compute model fits for a particular value of $\lambda$ that is not one of the original `grid` values. Note that by default, the `glmnet()` function standardizes the variables so that they are on the same scale. To turn off this default setting, use the argument `standardize=FALSE`. 

Associated with each value of $\lambda$ is a vector of ridge regression coefficients, stored in a matrix that can be accessed by `coef()`. In this case, it is a 20x100 matrix, with 20 rows (one for each predictor, plus an intercept) and 100 columns (one for each value of $\lambda$). 

```{r}
dim(coef(ridge.mod))
```

We expect the coefficient estimates to be smaller, in terms of $l_{2}$ norm, when a large value of $\lambda$ is used, as compared to when a small value of $\lambda$ is used. These are the coefficients when $\lambda$ = 11,498, along with their $l_{2}$ norm:

```{r}
# Find the 50th and 100th lambdas, along with the coefficients associated with that model and their l_2 norm (excluding the intercept)
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50])^2)
```

In contrast, here are the coefficients when $\lambda$ = 0.01, along with their $l_{2}$ norm. Note that much larger $l_{2}$ norm of the coefficients associated with this smaller value of $\lambda$. 

```{r}
ridge.mod$lambda[100]
coef(ridge.mod)[,100]
sqrt(sum(coef(ridge.mod)[-1,100])^2)
```

```{r}
# Just for fun, let's practice writing a function to extract this information for any lambda
shrinkage_coef <- function(glmnet_mod, ld) {
  coef_names <- names(coef(glmnet_mod)[,50])
  return(list(print(paste("The Lambda value is:",glmnet_mod$lambda[ld])),
         print(paste("The coefficient for",coef_names,"is:", coef(glmnet_mod)[,ld])),
         print(paste("The l_2 norm of these coefficients is:", sqrt(sum(coef(glmnet_mod)[-1,ld])^2)))
         ))
}
```

We now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso. There are two common ways to randomly split a dataset. The first is to produce a random vector of `TRUE`, `FALSE` elements and select the observations corresponding to `TRUE` for the training data. The second is to randomly choose a subset of numbers between 1 and n; these can then be used as the indices for the training observations.  The two approaches work equally well. We used the former method above. Here we demonstrate the latter approach. 

We first set a random seed so that the results obtained will be reproducible. 

```{r}
# Now that we've got a practice run down, let's split our sample to do some testing 
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
```

There are two common ways to randomly split a dataset. You can produce a random logical vector (`TRUE`/`FALSE`) and select observations corresponding to `TRUE` for the training data. Alternatively, randomly choose a subset of numbers between 1 and n, which can then be used as the indices for the training data. We do (and have been doing) the former in previous labs; this lab makes use of the latter. 

Next we fit a ridge regression model on the training set, and evaluate its MSE on th test set, using $\lambda$ = 4. Note the use of the `predict()` function again. This time we get predictions for a test set, by replacing `type="coefficients"` with the `newx` argument. 

```{r}
# Fit a ridge regression on the training set and evaluate its MSE on the test, using lambda = 4
ridge_mod <- glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge_pred <- predict(ridge_mod, s=4, newx=x[test,]) #s option sets the lambda value, newx specifies new observations used to make predicitons
mean((ridge_pred - y.test)^2) #MSE
```

The test MSE is 142199. Note that if we had instead simply fit a model with just an intercept, we would have predicted  each test observation using the mean of the training observations. In that case, we could compute the test set MSE like this:

```{r}
# Compare to the test MSE when lambda is extremely large (coefficients are approximately 0)
ridge_pred <- predict(ridge_mod, s=1e10, newx=x[test,])
mean((ridge_pred - y.test)^2)
```

We could also get the same result by fitting a ridge regression model with a *very* large value of $\lambda$. Bote that `1e10` means $10^{10}$. 

```{r}
# Finally, let's check if the ridge regression gives us better results than the least squares option (lambda = 0)
ridge_pred <- predict(ridge_mod, s=0, newx=x[test,]) #the exact option allow us to specify that lambda is exactly 0, instead of searching for the smalles value of lambda in "grid"
mean((ridge_pred - y.test)^2)
```

So fitting a ridge regression with $\lambda$ = 4 leads to a much lower test MSE than fitting a model with just an intercept. We now check whether there is any benefit  to performing ridge regression with $\lambda$ = 4 instead of just performing least squares regression. Recall that least squares is simply ridge regression with $\lambda$ = 0. 

In general, we want to fit a unpenalized least squares model, then we should use the `lm()` function, since that function provides more useful outputs, such as standard errors and p-values for the coefficients. 

In general, instead of arbitrarily choosing $\lambda$ = 4, it would be better to use cross-validation to choose the tuning parameter $\lambda$. We can do this using the built-in cross-validation function, `cv.glmnet()`. By default, the function performs ten-fold cross validation, through this can be changed using the argument `nfolds`. Note that we set a random seed first so our results will be reproducible, since the choice of the cross-validation folds is random. 

```{r}
# Instead of arbitrarily choosing a lambda value a prior, it is better to use cross-validation to find the best lambda
# This can be done using cv.glmnet(), which conducts 10-fold CV and can be increased to n-folds with the option nfolds
set.seed(1)
cv_out <- cv.glmnet(x[train,], y[train], alpha=0)
plot(cv_out)
```

```{r}
best_lam <- cv_out$lambda.min
best_lam
```

Therefore, we see that the value of $\lambda$ that results in the smallest cross-validation 326. What is the test MSE associated with the value of $\lambda$?

```{r}
# What is the test MSE associated with this lambda?
ridge_pred <- predict(ridge_mod, s=best_lam, newx=x[test,])
mean((ridge_pred - y.test)^2)
```

This represents a further improvement over the test MSE that we got using $\lambda$ = 4. Finally, we refit our ridge regression model on the full dataset, using the value of $\lambda$ chosen by cross-validation, and examine the coefficient estimates. 

```{r}
# Finally we can run ridge regression on the entire dataset, now that we have found the best value for our tuning parameter
out <- glmnet(x, y, alpha=0)
predict(out, type="coefficients", s=best_lam)[1:20,]
```

As expexted, none of the coefficients are zero - ridge regression does not perform variable selection!

# The Lasso

We saw that ridge regression with a wise choice of $\lambda$ can outperform least squares as well as the null model on the `Hitters` dataset. We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. In order to fit a lasso model, we once again use the `glmnet()` function; however, this time we use the argument `alpha=1`. Other than that change, we proceed just as we did in fitting a ridge model. 

```{r}
# Fit the lasso model and observe how some of the coefficients are exactly 0
lasso_mod <- glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso_mod)
```

We can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero. We now perform cross-validation and compute the associated test error. 

```{r}
# Perform CV and compute test errors
set.seed(1)
cv_out <- cv.glmnet(x[train,], y[train], alpha=1)
plot(cv_out)
```

```{r}
best_lam <- cv_out$lambda.min
lasso_pred <- predict(lasso_mod, s=best_lam, newx=x[test,])
mean((lasso_pred - y.test)^2)
```

This is substantially lower than the test set MSE of the null model and of least squares, and very similiar to the test MSE of ridge regression with $\lambda$ chosen by cross-validation. 

However, the lasso has a substantial advantage over ridge regression in that the result coefficient estimates are sparse. Here we see that 8 of the 19 coefficient estimates are exactly zero. So the lasso model with $\lambda$ chosen by cross-validation contains only seven variables. 

```{r}
# Fit the lasso over the entire dataset 
out <- glmnet(x, y, alpha=1, lambda=grid)
lasso_coef <- predict(out, type="coefficients", s=best_lam)[1:20,]
lasso_coef
lasso_coef[lasso_coef!=0]
```

The test MSE for the lasso is very similar to the ridge regression, but it does have a minor advantage: 8 of the 19 coefficients in the lasso model are exactly zero. 

# Principal Components Regression (PCR)

Principal components regression (PCR) can be performed using the `pcr()` function, which is part of the `pls` library. We now apply PCR to the `Hitters` data, in order to predict `salary`. Afain, ensure that the missing values have been removed from the data. 

```{r}
set.seed(2)
pcr.fit <- pcr(salary ~ ., data=hitters, scale=TRUE, validation="CV")
```

The syntax for the `pcr()` function is similiar to that for `lm()`, with a few additional options. Setting `scale=TRUE` has the effect of *standardizing* each predictor, prior to generating the principal components, so that the scale on which each variable is measured will not have an effct. Setting `validation="CV"` causes `pcr()` to compute the ten-fold cross-validation error for each possible value of *M*, the number of principal components used. The resulting fit can be examined using `summary()`. 

```{r}
summary(pcr.fit)
```

The CV score is provided for each possible number of components, ranging from M = 0 onwards. Note that `pcr()` reports the *root mean squared error*; in order to obtain the usual MSE, we must square this quantity. For instance, a root mean squared error of 352.8 corresponds to an MSE of $352.8^{2}$ = 124,468. 

One cal also plot the cross-validation scores using the `validationplot()` function. Using `val.type="MSEP"` will cause the cross-validation MSE to be plotted. 

```{r}
validationplot(pcr.fit, val.type="MSEP")
```

We see that the smallest cross-validation error occurs when M = 16 components are used. This is barely fewer than M = 19, which amounts to simply performing least squares, because when all of the components are used in PCR no dimension reduction occurs. However, from the plot we also see that the cross-validation error is rougly the same when only one component is included in the model. This suggests that a model that uses just a small number of components might suffice. 

The `summary()` function also provides the *percentage of variance explained* in the predictors and in the response using different numbers of components. Briefly, we can think of this as the amount of information about the predictors or the response that is captured using M principal components. For example, setting M = 1 only captures 38.31% of all the variance, or information, in the predictors. In contrast, using M = 6 increases the value to 88.63%. If we were to use all M = p = 19 components, this would increase to 100%. 

We now perform PCR on the training data and evaluate its test set performance. 

```{r}
set.seed(1)
pcr.fit = pcr(salary ~ ., data=hitters, subset=train, scale=TRUE, validation ="CV")
validationplot(pcr.fit,val.type="MSEP")
```

Now we find that the lowest cross-validation errors occurs when M = 5 components are used. We compute the test MSE as follows:

```{r}
pcr.pred = predict(pcr.fit, x[test,], ncomp=7)
mean((pcr.pred = y.test)^2)
```

The test set MSE is competitive with the results obtained using ridge regression and the lasso. However, as a result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform any kind of variable selection or even directly produce coefficient estimates. 

Finally, we fit PCR on the full dataset, using M = 7, the number of components identified by cross-validation. 

```{r}
pcr.fit = pcr(y~x, scale=TRUE, ncomp=7)
summary(pcr.fit)
```

# Partial Least Squares (PLS)

We implement partial least squares (PLS) using the `plar()` function, also in the `pls` library. The syntax is just like that of the `pcr()` function. 

```{r}
set.seed(1)
pls.fit = plsr(salary ~ ., data=hitters, subset=train, scale=TRUE, validation="CV")
summary(pls.fit)
```

The lowest cross-validation error occurs when only M = 2 partial least squares directions are used. We now evaluate the corresponding test set MSE. 

```{r}
pls.pred = predict(pls.fit, x[test,], ncomp=2)
mean((pls.pred - y.test)^2)
```

The test MSE is comparable to, but slightly higher than, the test MSE obtained using ridge regression, the lasso, and PCR. 

Finally, we perform PLS using the full dataset, using M = 2, the number of components identified by cross-validation. 

```{r}
pls.fit = plsr(salary ~ ., data=hitters, scale=TRUE, ncomp=2)
summary(pls.fit)
```

Notice that the percentage of variance in `Salary` that the two-component PLS fit explains, 46.40% is almost as much as that explained using the final seven-component model PCR fit, 46.69%. This is because PCR only attempts to maximize the amount of variance explained in the predictors, while PLS searches for directions that explain variance in both the predictors and the response. 