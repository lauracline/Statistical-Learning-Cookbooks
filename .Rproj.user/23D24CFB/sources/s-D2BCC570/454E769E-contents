---
title: "Linear Regression"
author: "Laura Cline"
date: "23/10/2021"
output:
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(MASS)
library(ISLR)
library(car)
```

# Simple Linear Regression 

The `MASS` library contains the `Boston` dataset, which records `medv` (median house value) for 500 neighbourhoods in Boston. We will seek to predict `medv` using 13 predictors such as `rm` (average number of rooms per house), `age` (average age of house), and `lstat` (percent of households with low socioeconomic status). 

```{r}
# Load our dataset and take a quick glance at its structure
names(Boston)
```

To find out more about the dataset, we can type `?Boston`. 

We will start by using the `lm()` function to fit a simple linear regression model, with `medv` as the response and `lstat` as the predictor. The basic syntax is `lm(y~x,data`), where `y` is the response, `x` is the predictor, and `data` is the dataset in which these two variables are kept. 

```{r}
# Start with a basic regression using the lm(y ~ x, data) function
lm.fit <- lm(medv ~ lstat, data=Boston)
```

If we type `lm.fit()`, some basic information about the model is output. For more detailed information, we use `summary(lm.fit)`. This gives us the p-values and standard errors for the coefficients, as well as the $R^{2}$ statisic and F-statistic for the model. 

```{r}
# To get detailed results of the regression, we use summary() on the lm object
summary(lm.fit)
```

We can use the `names()` function in order to find out what other pieces of information are stored in `lm.fit`. Although we can extract these quantities by name - e.g., `lm.fit$coefficients$ - it is safety to use the extractor functions like `coef()` to access them. 

```{r}
# The lm object contains quite a bit of information, we can use names() to parse it out and find out what we want
names(lm.fit)
```

```{r}
# An obvious result of interest is the coefficient values; we can call on them by name (lm.fit$coefficient) or use coef()
coef(lm.fit)
```

In order to obtain a confidence interval for the coefficient estimates, we can use the `confint()` command. 

```{r}
# for confidence intervals around these estimates, use the confint() function
confint(lm.fit)
```

The `predict()` function can be used to produce confidence intervals and prediction intervals for the prediction of `medv` for a given value of `lstat`. 

```{r}
# predict() is used to produce confidence intervals and prediction intervals for the prediction of medv for a fiven value of lstat 
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval="confidence")
```

For instance, the 95% confidence interval associated with an `lstat` value of 10 is (24.47, 25.63). As expected, the confidence intervals is centered around the point (a predicted value of 25.05 for `medv` when `lstat` equals 10), but the latter are substantially wider. 

We will now plot `medv` and `lstat` along with the least squares regression line using `plot()` and `abline()` functions. 

```{r}
# Let's visualize this least squares regression line
attach(Boston)
plot(lstat, medv)
abline(lm.fit)
```

There is some evidence for non-linearity in the relationship between `lstat` and `medv`. We will explore this in a later cookbook. 

The `abline()` function can be used to draw any line, not just the least squares regression line. To draw a line with intercept `a` and slope `b`, we type `abline(a,b)`. Below we experiment with some additinal settings for plotting lines and points. the `lwd = 3` command causes the width of the regression line to be increased be a factor of 3; this works for the `plot()` and `lines()` functions also. We can also use the `pch` option to create different plotting symbols. 

```{r}
# abline() allows us to draw any line, so play around with the options lwd (length), col (colour), and pch (plotting symbol)
plot(lstat, medv)
abline(lm.fit, lwd=3)
abline(lm.fit, lwd=3, col="red")
plot(lstat, medv, col="red")
plot(lstat, medv, pch=20)
plot(lstat, medv, pch="+")
plot(1:20, 1:20, pch=1:20)
```

Next we examine some diagnostic plots. Four diagnostic plots are automatically produced by applying the `plot()` function directly to the output of `lm()`. In general, this command will produce one plot at a time, and hitting *Enter* will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the `par()` function, which tells `R` to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, `par(mfrow=c(2,2))` divides the plotting region into a 2x2 grid of panels. 

```{r}
# The lm() function automatically produces diagnostic plots, which we can see my using plot()
# Instead of viewing the four graphs individually, we can tell R to split the display into 4 plots
par(mfrow=c(2,2))
plot(lm.fit)
```

Alternatively, we can compute the residuals from a linear regression fit using the `residuals()` function. The function `rstudent()` will return the standardized student residuals, and we can use this function to plot the residuals against the fitted values. 

```{r}
# Residuals can be plotted individually with residuals() or rstudent()
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
```

On the basis of the residuals plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the `hatvalues()` function. 

```{r}
# Leverage statistics can be computed for our predictors with hatvalues(); to find the observation with the largest leverage which.max() is used
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

The `which.max()` function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic. 

# Multiple Linear Regression 

In order to fit a multiple linear regression model using least squares, we again use the `lm()` function. The syntax `lm(y~x1+x2+x3)`is used to fit a model with three predictors, `x1`, `x2` and `x3`. The `summary()` function now outputs the regression coefficients for all the predictors. 

```{r}
# In order to add more predictors to our model, we use '+' in the lm syntax
lm.fit <- lm(medv ~ lstat + age, data=Boston)
summary(lm.fit)
```

The `Boston` dataset contains 13 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all the predictors. Instead, we can use the following short-hand:

```{r}
# It is tedious to type out every one of our predictors, so we can use "." on the right hand side of the regression equation 
lm.fit <- lm(medv ~ ., data=Boston)
summary(lm.fit)
```

We can access the individual components of a summary object by name (type `?summary.lm` to see what is available). Hence `summary(lm.fit)$r.sq` gives us the $R^{2}$, and `summary(lm.fit)$sigma` gives us the RSE. The `vif()` function, part of the `car` package, can be used to compute variance inflation factors. Most VIFs are low to moderate for this data. 

```{r}
# Check out the R^2 and Residual Squared Error of this model
summary(lm.fit)$r.sq
summary(lm.fit)$sigma
```

```{r}
# Variance Inflation Factor (VIF) is found with vif() in the "car" package
vif(lm.fit)
```

What if we would like to perform a regression using all of the variables but one? For example, in the above regression output, `age` has a high p-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except `age`. 

Alternatively, the `update()` function can be used. 

```{r eval=FALSE, include=FALSE}
# If we want to exclude one or more of the predictors from our model, we use -varname on the right hand side of the equation 
lm.fit1 <- lm(medv ~ . -age, data = Boston)
summary(lm.fit1)
lm.fit1 <- update(lm.fit ~ .-age)
```

# Interaction Terms 

It is easy to include interaction terms in a linear model using the `lm()` function. The syntax `lstat:black` tells `R` to include an interaction term between `lstat` and `black`. The syntax `lstat*age` simultaneously includes `lstat`, `age` and the interaction term `lstat x age` as predictors; it is shorthand for `lstat + age + lstat:age`. 

```{r}
# Finally, we can include interaction terms in the model by multiplying the two variables of interest within the `lm()` function
summary(lm(medv ~ lstat*age, data=Boston))
```

# Non-Linear Transformations of the Predictors 

The `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor X, we can create a predictor $X^{2}$ using `I(X^2)`. The function `I()` is needed since the `^` has a special meaning in a formula; wrapping as we do allows the standard usage in `R`, which is to raise `X` to the power of `2`. We now perform a regression of `medv` onto `lstat` and `lstat^{2}`

```{r}
# lm() can accommodate for transformations of the predictors using I(varname)
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)
```

The near-zero p-value associated with the quadratic term suggests that it leads to an improved model. We use the `anova()` function to further quantify the extent to which the quadratic fit is superior to the linear fit. 

```{r}
# To quantify how much better the quadratic terms fit the model, use anova()
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)
```

Here Model 1 represents the linear submodel containing only one predictor, `lstat`, while Model 2 corresponds to the larger quadratic model that has two predictors `lstat` and `lstat^2`. The `anova()` function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Gere, the F-Statistic is 135.2 and the associated p-value is virtually zero. This provides clear evidence that the model containing the predictors `lstat` and `lstat^2` is far superior to the model that only contains the predictor `lstat`. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. If we type:

```{r}
# For more evidence, let's visualize the residuals
par(mfrow=c(2,2))
plot(lm.fit2)
```

then we see that when the `lstat^2` term is included in the model, there is little discernable pattern between the residuals. 

In order to create a cubic fit, we can include a predictor of the form `I(X^3)`. However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the `poly()` function to create the polynomial within `lm()`. For example, the following command produces a fifth-order polynomial fit:

```{r}
# We can include higher order polynomials by using poly(var, degree) within the lm function
lm.fit5 <- lm(medv ~ poly(lstat,5))
summary(lm.fit5)
```

This suggests that including additional polynomials terms, up to the fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond the fifth order have signficant p-values in a regression fit. 

Of course, we are in no way restricted to using polynomial transformations of predictors. Here we try a log transformation. 

```{r}
# Last, but not least, check out a log transformation of our predictor "rm"
summary(lm(medv~ log(rm), data=Boston))
```

# Qualitative Predictors 

We will now examine the `Carseats` data, which is part of the `ISLR` library. We will attempt to predict `Sales` (child car seat sales) in 400 locations based on a number of predictors. 

```{r}
names(Carseats)
```

The Carseats data includes qualitative predictors such as `Sleveloc`, an indicator of the quality of the shelving location - that is, the space within a store in which the car seat is displayed - at each location. The predictor `Sleveloc` takes on tree possible values, *Bad, Medium* and *Good*. 

Given a qualitative variable such as `Shelveloc`, `R` generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms. 

```{r}
# Fortunately, R is able to recognize categorical variables and will automatically generate dummies when we run a regression 
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, data=Carseats) # Using ":" is another way to include interactions
summary(lm.fit)
```

The `contrasts()` function returns the coding that `R` uses for the dummy variables. 

```{r}
# The contrasts() function returns the coding that R uses for the dummy variables
attach(Carseats)
contrasts(ShelveLoc)
```

Use `?contrasts` to learn about other contrasts and how to set them. 

`R` has create a `ShelveLocGood` dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a `ShelveLocMedium` dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for `ShelveLocGood` in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And `ShelveLocMedium` has a smaller positive coefficient, indicating that a medium shelving location leads to higher sales than a bad shelving location but lower sales than a good shelving location. 