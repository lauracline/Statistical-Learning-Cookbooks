{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised Learning",
      "provenance": [],
      "authorship_tag": "ABX9TyPX2EXgw4IIqQ4sQz+piWyD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lauracline/Statistical-Learning-Cookbooks/blob/master/Unsupervised_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqc2_qcGhBNo"
      },
      "source": [
        "# **Unsupervised Learning**\n",
        "\n",
        "This book has been about supervised learning until now. No response variable will try and find interesting things in explanatory variables. This chapter will focus on principal components analysis and clustering. There is not always an exact goal. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uDafUc3hPDa"
      },
      "source": [
        "## **Principal Components Analysis**\n",
        "\n",
        "We previously covered principal component regression where the original features were mapped to a smaller feature space that are the used as inputs into linear regression solved normally through least squares. \n",
        "\n",
        "PCA can be used to visualize high dimensional data in 2 or 3 dimensions. The first component is a weighted linear combination of all the original features where the sum of the squared weights equals 1. These weights are the loading factors. The loading factors of the first principal component maximize the weighted sum of the features for each observation. \n",
        "\n",
        "The second principal component is uncorrelated with the first which makes it orthogonal to it. \n",
        "\n",
        "The first principal component can be interested as the line closest to te data. \n",
        "\n",
        "It is very important to scale the data first - mean of 0 and a standard deviation of 1. The variances won't make sense otherwise. \n",
        "\n",
        "### **Proportion of Variance Explained**\n",
        "\n",
        "Each principal component explains some of the vriance of the original data. We can find the proportion that each principal component explains by dividing each component's variance by the total raw variance. Summing all the variances for each component equals 1. \n",
        "\n",
        "Examine a scree plot (fot an elbow) to choose the number of principal components to use. Or you can use cross-validation to choose. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ2txN8AiNQL"
      },
      "source": [
        "## **Clustering**\n",
        "\n",
        "Finding groups within data that are similiar. We can cluster by using the features or by the observation (transposing the data matrix). \n",
        "\n",
        "### **K-Means**\n",
        "\n",
        "Clustering where you define the number of clusters ahead of time. Algorithm works interatively by first randomly assigning each point to a cluster and computing cluster centers. All points are then reassigned based on the Euclidean distance to centroids. A new centroid is found by averaging the point of each cluster. The process stops after centroids stop moving or some max number of iterations. \n",
        "\n",
        "We can do the initial assignment multiple times and choose the clustering assignment with the total least variance. \n",
        "\n",
        "### **Hierarchical Clustering**\n",
        "\n",
        "No need to pre-specify the number of clusters. The most common type is bottom-up or agglomerative. \n",
        "\n",
        "#### **Interpreting Dendrograms**\n",
        "\n",
        "Similarity of points should be determined by the vertical axis not the horizonal axis. The lower on the dendrogram they are connected, the closer they are. \n",
        "\n",
        "Hierarchical clustering works by putting each point in its own cluster. Then each pairwise dissimilarity is compited and the least dissimilar clusters are fused. This dissimilarity is the height of the dendrogram. Dissimilarity is calculated through a type of linkage and distance metric (usually Euclidean). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oWhp8LxjKDN"
      },
      "source": [
        "## **Expectation Maximization**\n",
        "\n",
        "K-means and hierarchical clustering are 'hard' clustering meaning that each observation belongs to exactly one cluster. There are other clustering algorithms that 'soft'  cluster meaning that observations can belong to multiple clusters. \n",
        "\n",
        "One way to perform soft clustering is by assuming that each cluster is modeled by a normal (gaussian) distribution. And so the whole dataset is a mixture of gaussian also called the Gaussian Mixture Model. \n",
        "\n",
        "The goal here is to find the parameters for the multivariate gaussian and assign each observation a probability of being in a certain cluster. \n",
        "\n",
        "## **EM Algorithm**\n",
        "\n",
        "In K-means, we start the algorithm by randomly assigning each point to a cluster to find the first centroid. Somewhat similarily, the EM algorithm randomly assigns the parameters of a gaussian distribution to each cluster. Then uses bayes theorem (the initial priors are all equal), we can determine the probability of each point being part of a cluster. This is the expectation step. \n",
        "\n",
        "The maximization step is to recalculate the parameters (mean and covariance) of the multivariate gaussian distribution using the now weighted (by probabilities) observations. "
      ]
    }
  ]
}
